{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "image = \"../assets/car.jpg\"\n",
    "prompt = \"What is in the picture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The picture shows a vintage Volkswagen Beetle car parked on a street. The car is painted in a light turquoise color and is positioned in front of a yellow wall with wooden doors. The setting appears to be in a quaint, possibly historical or tourist area, given the style of the building and the car.']\n",
      "2.879515074\n",
      "6.510447801217525\n"
     ]
    }
   ],
   "source": [
    "# test qwen2VL\n",
    "from image_text_to_text import Qwen2VLExecutor\n",
    "tmp = Qwen2VLExecutor(\"Qwen/Qwen2-VL-7B-Instruct-AWQ\")\n",
    "inputs = {\n",
    "    \"prompt\": prompt,\n",
    "    \"image\": image,\n",
    "    \"max_output_size\": 1000\n",
    "}\n",
    "answer = tmp.generate_output(inputs)\n",
    "print(answer)\n",
    "print(tmp.get_latency())\n",
    "print(tmp.get_memory())\n",
    "# # print(tmp.get_pixels_processed())\n",
    "# # print(tmp.get_tokens_processed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test florence-2\n",
    "from executors import FlorenceExecutor\n",
    "model_name = \"microsoft/Florence-2-base\"\n",
    "tmp = FlorenceExecutor(model_name)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "def plot_bbox(image, data):\n",
    "   # Create a figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Display the image(file str)\n",
    "    img = plt.imread(image) \n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Plot each bounding box\n",
    "    for bbox, label in zip(data['bboxes'], data['labels']):\n",
    "        # Unpack the bounding box coordinates\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        # Add the rectangle to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        # Annotate the label\n",
    "        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    # Remove the axis ticks and labels\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig('florence_test.jpg')\n",
    "\n",
    "# Caption\n",
    "task_prompt = '<CAPTION>'\n",
    "inputs = {\n",
    "    \"prompt\": task_prompt,\n",
    "    \"image\": image,\n",
    "    \"text\": \"\",\n",
    "    \"max_output_size\": 200\n",
    "}\n",
    "answer = tmp.generate_output(inputs)\n",
    "print(answer)\n",
    "task_prompt = '<DETAILED_CAPTION>'\n",
    "answer = tmp.generate_output(task_prompt, image)\n",
    "print(answer)\n",
    "task_prompt = '<MORE_DETAILED_CAPTION>'\n",
    "answer = tmp.generate_output(task_prompt, image)\n",
    "print(answer)\n",
    "\n",
    "# Object detection\n",
    "task_prompt = '<OD>'\n",
    "answer = tmp.generate_output(task_prompt, image)\n",
    "print(answer)\n",
    "plot_bbox(image, answer['<OD>'])\n",
    "\n",
    "task_prompt = \"<DENSE_REGION_CAPTION>\"\n",
    "answer = tmp.generate_output(task_prompt, image)\n",
    "print(answer)\n",
    "plot_bbox(image, answer['<DENSE_REGION_CAPTION>'])\n",
    "\n",
    "# ocr\n",
    "image = \"../assets/cuda.jpg\"\n",
    "task_prompt = '<OCR>'\n",
    "answer = tmp.generate_output(task_prompt, image)\n",
    "print(answer)\n",
    "task_prompt = '<OCR_WITH_REGION>'\n",
    "answer = tmp.generate_output(task_prompt, image)\n",
    "print(answer)\n",
    "\n",
    "# More Detailed Caption + Phrase Grounding \n",
    "task_prompt = '<MORE_DETAILED_CAPTION>'\n",
    "results = tmp.generate_output(task_prompt, image)\n",
    "text_input = results[task_prompt]\n",
    "task_prompt = '<CAPTION_TO_PHRASE_GROUNDING>'\n",
    "results = tmp.generate_output(task_prompt, image, text_input)\n",
    "results['<MORE_DETAILED_CAPTION>'] = text_input\n",
    "print(results)\n",
    "plot_bbox(image, results['<CAPTION_TO_PHRASE_GROUNDING>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  test llava\n",
    "from executors import LLaVAExecutor\n",
    "model_name = \"llava-hf/llava-1.5-13b-hf\"\n",
    "tmp = LLaVAExecutor(model_name)\n",
    "inputs = {\n",
    "    \"prompt\": prompt,\n",
    "    \"image\": image,\n",
    "    \"max_output_size\": 1000\n",
    "}\n",
    "results = tmp.generate_output(inputs)\n",
    "print(results)\n",
    "print(\"Inference latency is：\", tmp.get_latency(), \"s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
